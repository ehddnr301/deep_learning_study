# 1 주차

- 2주차 부터는 numpy 를 다룰예정
  - 행렬을 처리

## Deep learning

- Deep learning ? Machine learning

### Machine learning

- 기존프로그래밍

  - 기존방식은 input data를 넣어주고 f 함수를 만들어주면 result 발생
  - 스팸메일 거르기 : 규칙을 주고(f), input data 는 모든 메일 => result 발생

- x(input data)랑 result를 주면 규칙을 학습
- data만 많이주면 알아서 학습
- data를 주면 rule을 찾음

### Deep learning

- AI > Machine learning(이미지쪽에서 학습이 잘 안됨.), Nerual Network가 성능이 좋더라
- Nerual Network을 여러개 쌓으면 성능이 향상되더라.

## 강의듣기

- Machine learning(ML)

### 용어와 개념

- x는 input value y는 result
- ML : 데이터를 많이주면 스스로 규칙을 학습하게 한다.
- Supervised learning : 라벨이 붙은 이미지를 학습
  - 지도학습이라는 의미 = x랑 y를 주면 규칙을 발견하는 방식
  - 정답이 존재하면 그 규칙을 발견하는 방식
  - `label`이라는 지도자가 존재
  - 현재 산업 대부분
  - data가 labeled data
  - x,y 쌍으로 존재함
- Unsupervised learning : 데이터를 보고 스스로 학습
  - `label` 이 없음
  - 뉴스를 굉장히 많이 주면 그 뉴스중에 유사한것을 찾아서 label을 찾아가서 묶어줌
  - x만 존재

#### Supervised learning

##### Training data

- labeled data 와 동의어
- 훈련을 위한 데이터
  - 9시간 공부 100점
  - 5시간 공부 40점
  - 2시간 공부 10점

##### train model

- trained model

##### unseen data x

- label되지 않은 데이터를 넣어주면 예측
  - 4시간 공부? 30점!

##### 분류

- y의 종류에 따라 분류

  - label : 성별
  - class : 남, 녀 2가지

- regression

  - y의 종류가 continuous : 기온 예측

- y의 종류가 discrete 한 경우

  - Binary classification

    - 인물사진으로 성별 예측
    - pass/non-pass 예측,

  - Multi-class classification
    - 숫자 인식
    - y의 클래스가 2가지 이상

## 강의

- 함수를 여러개 쌓는다 = 합성함수 = 딥러닝
- NN = ML 의 기법중 하나
- NN 을 깊게 쌓으면 DNN
- 딥러닝 = 거대한 함수

lecture 2-4

### 2

- data 개수는 m 으로 표현
- 6개의 집 데이터 가 있으면
- 주택의 크기가 x (feature)
- 주택의 가격이 y (label)
- ReLu
- family size
- 한개의 뉴런: size => 뉴런 => price
- 한개의 뉴런: 침대수 => 뉴런 => price
- school quality
- 한개의 뉴런: zipcode => 뉴런 => price
- 한개의 뉴런: 지역부유함정도 => 뉴런 => price

- size #bedrooms zip code wealth 등으로 y 예측
- x와 y 사이에 hidden node? hidden unit이 존재 ( family size 같은 것일수 있지만 NN 에서는 정확히 무엇이라고 정의할수 없음 ) ( 딥러닝에서 결정 ) (블랙박스모델)
- 내부에서 어떤 일이 일어나는지 알수없다.
  - 잘못된 모델을 만들면 책임소재를 찾기가 힘듬
- DL 은 필요없는 feature의 가중치를 죽인다.

### 3

- Structured Data : Excel로 정의할수있음 (나이, 사이즈, 베드룸숫자 등으로 가격 도출)
- Unstructured Data : 이미지나 문자열 인식
- 이미지 ? : 데이터를 어떻게 넣어줘야할지 잘모르겠음 ( 이미지는 숫자가 아니잖아 ), convolution NN
- 음성 : 음성인식가능
- 텍스트 : recurrent NN
- 기온 : sequential 이지만 structured data
- 오디오나 텍스트 : sequential 하지 않지만 unstructured data

### 4

- scale

  - 모델의 크기가 커지면 연산하는데 시간이 오래걸림. ( 학습에 시간이 많이 소요됨 )
  - 데이터 양이 많아지면 좋은데 데이터는 한정적이다.

- 해결책
  - 왜 신경망이 최근에서야 성공했는지
  - 데이터도 많아지고
  - 컴퓨테이션 gpu 좋아짐
  - 알고리즘 개선
    - sigmoid -> relu

### Linear Regression의 hypothesis 와 cost

- 딥러닝에서 학습이란 어떤 방식으로 일어나는가

  - 리그레션 관점에서 알아보자

- 모두를 위한 DL
  - 학습이라는 걸 할떄 기계한테 알려줘야함 어떤모델이 좋은모델인지 안좋은 모델인지 정도는 알려줘야함.
  - 좋고 나쁨을 수치화 시켜줘야함

```
x | y
1 | 1
2 | 2
3 | 3
```

- ML : x와 y의 관계인 f를 찾는 과정.
- 그래프를 그리면 기울기가 1에서 3개의 점이 찍힐것이다.
- linear 가설 : 데이터에 맞는 선을 찾는 것
- H(x) = Wx + b 라는 직선 공식이 나올것 ( 1차 방정식 )
- 이렇게 직선 공식을 구했는데 어떤 선이 우리의 가설과 잘 맞는 걸까!?
- 실제 데이터와 가설의 점의 거리를 비교해서 계산하면 된다.
- cost function
  - how fit the line to our data
  - 가설과 우리 데이터가 얼마나 다른가
    - ( H(x) - y )^2 => 차이를 항상 양수로 표현해줌, 차이가 클때 패널티를 크게 줌
    - 코스트 펑션을 최소화하는게 목적
- W,x,b,y 중에 x,y는 주어짐 그럼 W,b의 함수가 된다.
- W는 가중치를 의미
- cost function 값이 제일 작을수 있는 W와 b의 값을 찾는 과정을 학습이라고 한다.

### gradient descent

- 경사하강 검사법
- cost function을 최소화
- cost(W,b) 가 있을때 이것을 적용하면 최소화되는 W,b 를 결정할수있음.

- J = cost(w,b)
- 최소점을 어떻게 찾을까 ( gradient descent 알고리즘 )

- 최소점 찾기
- 한라산을 내려오는데 어떻게하면 빨리 내려갈수있을까(경사가 있는곳으로 한발짝)
- 계속 경사가 가파른 곳으로 한발짝
- 그러다보면 평지로 내려오게됨.

- `현재 내가 있는 점에서 가장 많이 내려갈수있는곳으로 한발짝`
- 초기점을 무엇으로 잡느냐에 대한 이슈도 있음.
- `W = W - 알파 * dJ/dW`
- 알파 : learning rate : 학습속도
  - 학습속도가 빠르면 한번에 많이 내려가겠지
    - 최적점 찾기가 힘들수 있음.
  - 학습속도가 느리면 쪼금식 내려가겠지
    - 하지만 최적점 확실히 찾을수있음
- cost(W,b) 를 줄일수있는 곳으로 W와 b를 옮김 과정을 반복해서 0이되는 지점까지

- 최적점을 찾아가는데 그 방식이 다양함.
- 관성을 줘서 global minimum을 찾아갈수 있음.

- 내가 현재 위치한 방향대로 간다. 현재 기울기 방향대로 간다.

### convex

- 시작점에 따라 최적점이 달라질수있음.
- convex function
- 밥그릇형태라서 시작점을 어디로 잡던지 최적점이 같을것
