# 3주차 정리

- data 업로드 & 확인 작업이 다 끝났다면 그 이후에 진행되는 내용!

## 1. data Shape 찾기.

- `train_x_orig.shape = (m_train, num_px, num_px, 3)`
- `test_x_orig.shape = (m_test, num_px, num_px, 3)`
  - m_train (number of training examples)
  - m_test (number of test examples)
  - num_px (= height = width of a training image)

## 2. data preprocess

### flatten

- `train_flatten = train_x_orig.reshape(m_train,-1).T`
- `test_flatten = test_x_orig.reshape(m_test,-1).T`
  - (-1,m_train) 을 하지않는 이유는 순서가 꼬이기 때문입니다.
  - 순서가 꼬이면? : 이미지 픽셀이 꼬이면 우리가 원한 이미지가 아니게 됩니다.

### nomalization

- `train_set_x = train_set_x_flatten/255.`
- `test_set_x = test_set_x_flatten/255.`
  - pixel 이기 때문에 0부터 255의 범위를 가지게됩니다.
  - 0부터 255의 범위보다 0부터 1범위가 학습효과가 좋기떄문에 nomalization을 진행합니다.

## 3. 학습 알고리즘의 일반적인 형태

- Logistic Regression
- 하나의 x에 대한 식
  - `z = w.T * x + b`
  - `y햇 = a = sigmoid(z)`
  - `L(a, y) = -ylog(a) - (1-y)log(1-a)` | Loss function
  - `J = 1/m 인테그랄 ( L(a,y) )` | Cost function

### Neural Network 🚀

- Define the model structure
- initialize the model's parameter
- loop
  - Calculate current loss | A ( forward propagation )
  - Compute Cost function | J
  - Calculate current gradient ( backward propagation )
  - Update Parameters ( gradient discent )

## 4. Sigmoid function

- z에 저장된 결과를 0또는 1로 바꿀함수가 필요한데 0과1사이의 실수를 리턴하는 유명 한 함수가 sigmoid함수입니다. 뜻은 'S자모양'의 함수라는 뜻입니다.
- `sigmoid(z) = 1 / (1 + e^-z)`
- `s = 1 / (1 + np.exp(-z))`

## 5. Initializing parameters

- `w = np.zeros((dim,1))`
- `b = 0`

## 6. Forward & Backward propagation

### FORWARD PROPAGATION (FROM X TO COST)

- `A = sigmoid(np.dot(w.T,X)+b)` # compute activation
- `cost = (-1/m) * np.sum(Y * np.log(A) + (1-Y) * np.log(1-A))` # compute cost

### BACKWARD PROPAGATION (TO FIND GRAD)

- `dw = (1/m) \* np.dot(X ,(A-Y).T)`
- `db = (1/m) \* np.sum(A-Y)`

## 7. Optimization ( Gradient Discent )

- update parameters using gradient discent
- w -- weights, a numpy array of size (num_px x num_px x 3, 1)
- b -- bias, a scalar
- X -- data of shape (num_px x num_px x 3, number of examples)
- Y -- true "label" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)
- params -- dictionary containing the weights w and bias b
- grads -- dictionary containing the gradients of the weights and bias with respect to the cost function
- costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.

```
def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):

    costs = []

    for i in range(num_iterations):


        # Cost and gradient calculation (≈ 1-4 lines of code)
        grads, cost = propagate(w,b,X,Y)

        # Retrieve derivatives from grads
        dw = grads["dw"]
        db = grads["db"]

        # update rule (≈ 2 lines of code)
        w -= learning_rate * dw
        b -= learning_rate * db

        # Record the costs
        if i % 100 == 0:
            costs.append(cost)

        # Print the cost every 100 training iterations
        if print_cost and i % 100 == 0:
            print ("Cost after iteration %i: %f" %(i, cost))

    params = {"w": w,
              "b": b}

    grads = {"dw": dw,
             "db": db}

    return params, grads, costs
```

- 학습된 w와 b를 가집니다.

## 8. Predict

```
if A[0, i] > 0.5:
    Y_prediction[0, i] = 1
else:
    Y_prediction[0, i] = 0
```

- 나온 값이 0.5 초과이면 1로 0.5 이하이면 0으로 간주합니다.

# 최종

```
def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):

    # initialize parameters with zeros (≈ 1 line of code)
    w, b = initialize_with_zeros(train_set_x_flatten.shape[0])

    # Gradient descent (≈ 1 line of code)
    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)

    # Retrieve parameters w and b from dictionary "parameters"
    w = parameters["w"]
    b = parameters["b"]

    # Predict test/train set examples (≈ 2 lines of code)
    Y_prediction_test = predict(w,b,X_test)
    Y_prediction_train = predict(w,b,X_train)

    d = {"costs": costs,
         "Y_prediction_test": Y_prediction_test,
         "Y_prediction_train" : Y_prediction_train,
         "w" : w,
         "b" : b,
         "learning_rate" : learning_rate,
         "num_iterations": num_iterations}

    return d
```
