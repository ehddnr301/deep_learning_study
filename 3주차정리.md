# 3ì£¼ì°¨ ì •ë¦¬

- data ì—…ë¡œë“œ & í™•ì¸ ì‘ì—…ì´ ë‹¤ ëë‚¬ë‹¤ë©´ ê·¸ ì´í›„ì— ì§„í–‰ë˜ëŠ” ë‚´ìš©!

## 1. data Shape ì°¾ê¸°.

- `train_x_orig.shape = (m_train, num_px, num_px, 3)`
- `test_x_orig.shape = (m_test, num_px, num_px, 3)`
  - m_train (number of training examples)
  - m_test (number of test examples)
  - num_px (= height = width of a training image)

## 2. data preprocess

### flatten

- `train_flatten = train_x_orig.reshape(m_train,-1).T`
- `test_flatten = test_x_orig.reshape(m_test,-1).T`
  - (-1,m_train) ì„ í•˜ì§€ì•ŠëŠ” ì´ìœ ëŠ” ìˆœì„œê°€ ê¼¬ì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
  - ìˆœì„œê°€ ê¼¬ì´ë©´? : ì´ë¯¸ì§€ í”½ì…€ì´ ê¼¬ì´ë©´ ìš°ë¦¬ê°€ ì›í•œ ì´ë¯¸ì§€ê°€ ì•„ë‹ˆê²Œ ë©ë‹ˆë‹¤.

### nomalization

- `train_set_x = train_set_x_flatten/255.`
- `test_set_x = test_set_x_flatten/255.`
  - pixel ì´ê¸° ë•Œë¬¸ì— 0ë¶€í„° 255ì˜ ë²”ìœ„ë¥¼ ê°€ì§€ê²Œë©ë‹ˆë‹¤.
  - 0ë¶€í„° 255ì˜ ë²”ìœ„ë³´ë‹¤ 0ë¶€í„° 1ë²”ìœ„ê°€ í•™ìŠµíš¨ê³¼ê°€ ì¢‹ê¸°ë–„ë¬¸ì— nomalizationì„ ì§„í–‰í•©ë‹ˆë‹¤.

## 3. í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì˜ ì¼ë°˜ì ì¸ í˜•íƒœ

- Logistic Regression
- í•˜ë‚˜ì˜ xì— ëŒ€í•œ ì‹
  - `z = w.T * x + b`
  - `yí–‡ = a = sigmoid(z)`
  - `L(a, y) = -ylog(a) - (1-y)log(1-a)` | Loss function
  - `J = 1/m ì¸í…Œê·¸ë„ ( L(a,y) )` | Cost function

### Neural Network ğŸš€

- Define the model structure
- initialize the model's parameter
- loop
  - Calculate current loss | A ( forward propagation )
  - Compute Cost function | J
  - Calculate current gradient ( backward propagation )
  - Update Parameters ( gradient discent )

## 4. Sigmoid function

- zì— ì €ì¥ëœ ê²°ê³¼ë¥¼ 0ë˜ëŠ” 1ë¡œ ë°”ê¿€í•¨ìˆ˜ê°€ í•„ìš”í•œë° 0ê³¼1ì‚¬ì´ì˜ ì‹¤ìˆ˜ë¥¼ ë¦¬í„´í•˜ëŠ” ìœ ëª… í•œ í•¨ìˆ˜ê°€ sigmoidí•¨ìˆ˜ì…ë‹ˆë‹¤. ëœ»ì€ 'Sìëª¨ì–‘'ì˜ í•¨ìˆ˜ë¼ëŠ” ëœ»ì…ë‹ˆë‹¤.
- `sigmoid(z) = 1 / (1 + e^-z)`
- `s = 1 / (1 + np.exp(-z))`

## 5. Initializing parameters

- `w = np.zeros((dim,1))`
- `b = 0`

## 6. Forward & Backward propagation

### FORWARD PROPAGATION (FROM X TO COST)

- `A = sigmoid(np.dot(w.T,X)+b)` # compute activation
- `cost = (-1/m) * np.sum(Y * np.log(A) + (1-Y) * np.log(1-A))` # compute cost

### BACKWARD PROPAGATION (TO FIND GRAD)

- `dw = (1/m) \* np.dot(X ,(A-Y).T)`
- `db = (1/m) \* np.sum(A-Y)`

## 7. Optimization ( Gradient Discent )

- update parameters using gradient discent
- w -- weights, a numpy array of size (num_px x num_px x 3, 1)
- b -- bias, a scalar
- X -- data of shape (num_px x num_px x 3, number of examples)
- Y -- true "label" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)
- params -- dictionary containing the weights w and bias b
- grads -- dictionary containing the gradients of the weights and bias with respect to the cost function
- costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.

```
def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):

    costs = []

    for i in range(num_iterations):


        # Cost and gradient calculation (â‰ˆ 1-4 lines of code)
        grads, cost = propagate(w,b,X,Y)

        # Retrieve derivatives from grads
        dw = grads["dw"]
        db = grads["db"]

        # update rule (â‰ˆ 2 lines of code)
        w -= learning_rate * dw
        b -= learning_rate * db

        # Record the costs
        if i % 100 == 0:
            costs.append(cost)

        # Print the cost every 100 training iterations
        if print_cost and i % 100 == 0:
            print ("Cost after iteration %i: %f" %(i, cost))

    params = {"w": w,
              "b": b}

    grads = {"dw": dw,
             "db": db}

    return params, grads, costs
```

- í•™ìŠµëœ wì™€ bë¥¼ ê°€ì§‘ë‹ˆë‹¤.

## 8. Predict

```
if A[0, i] > 0.5:
    Y_prediction[0, i] = 1
else:
    Y_prediction[0, i] = 0
```

- ë‚˜ì˜¨ ê°’ì´ 0.5 ì´ˆê³¼ì´ë©´ 1ë¡œ 0.5 ì´í•˜ì´ë©´ 0ìœ¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.

# ìµœì¢…

```
def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):

    # initialize parameters with zeros (â‰ˆ 1 line of code)
    w, b = initialize_with_zeros(train_set_x_flatten.shape[0])

    # Gradient descent (â‰ˆ 1 line of code)
    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)

    # Retrieve parameters w and b from dictionary "parameters"
    w = parameters["w"]
    b = parameters["b"]

    # Predict test/train set examples (â‰ˆ 2 lines of code)
    Y_prediction_test = predict(w,b,X_test)
    Y_prediction_train = predict(w,b,X_train)

    d = {"costs": costs,
         "Y_prediction_test": Y_prediction_test,
         "Y_prediction_train" : Y_prediction_train,
         "w" : w,
         "b" : b,
         "learning_rate" : learning_rate,
         "num_iterations": num_iterations}

    return d
```
