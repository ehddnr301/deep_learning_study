# 11

## Batch Normalization

- 왜 batch ? 데이터를 다 쓸수 없음
- 분포를 최적의 분포로 맞춰줌.
- 그래서 batch별로 구해서 평균을 냄
- input normalization
  - 스케일을 일정하게 맞춰주려고
- batch? normalization

  - layer 중간중간 normalization을 해줌.
  - 이걸하면 뭐가 좋을까 ?
  - 보다 효과적일수 있음.

- z만 normalization

- 표준정규분포 - 평균 0 분산 1 이 되도록 만들기

- 평균과 분산을 계산을 해서 표준화
- 1. z에서 평균빼고 분산으로 나눠주기 (표준정규분포가 나옴)
- 2. 감마를 곱해주고 베타를 더해준다. (표준정규분포 값에 상수를 곱하고 상수를 더해준다)
- 3. 그럼 평균이 베타이고 분산이 감마^2 인 정규분포를 따른다.

  - 베타, 감마는 parameter 라서 알아서 학습하는 요소임.

- BN 은 batch에서 channel 별로 일어난다. 그래서 shape: D 이면 2D개가 있다 ?

- 뮤, 시그마 : Learnable x
- 감마, 베타 : Learnable parameter

### BN 이 없으면 ?

- Internal Covariate Shift : 내부, 공변량, 변화
- 내부 : model 내부 layer 사이
- 공변량 : 공분산 - 분포

  - layer사이의 분포가 바뀐다 : 이게 왜 문제 ??

- w1,b1 에서 w2,b2 로 넘어가는게 불안정함
- distribution이 불안정한 input을 받는다.
- FP -> gradient descent -> w,b update -> a update

### BN

- 즉 z를 특정분포 (N(베타,감마^2)) 로 제한
- w, b update를 특정 분포 내에서 되게 제한

  - 앞쪽 파라미터와 뒤쪽 파라미터가 덜 연관되게 바꿔줌

- 장점 : internal covariate shift 해결 : 학습 잘됨
- gradient vanishing / exploding 방지
- 미세한 regularization 효과

## Normalization layers

- Batch Normalization
- 채널단위로 평균과 표준편차를 구함

### 코드로 구현

- 쓰고 싶은 레이어 뒤에 BatchNormalization() 을 더해주면 됨.

### 단점

- batch 단위로 이루어지기 때문에 batch_size가 작으면 평균,분산이 너무 부정확하다.
- RNN은 같은 layer 에서도 cell 이동에 따라 다른 feature들을 받으므로 통계량이 매번 다르다.

### layer normalization

- 배치는 channel을 기준으로 , layer는 batch를 기준으로
- 구현 LayerNomalization()

## transfer learning

- 큰 모델을 train하려면 많은 data가 필요
- overfitting을 막기위해 많은 data가 필요

- train에 소요되는 시간
  - 데이터가 많을수록 train에 많은 시간이 걸립니다.

### 사용법

- 모델별로 독자적인 preprocess 방법이 따로 있음.
- trainable = False 면 freeze 시키는거임

### 특징

- pretrained 모델에서 구조를 그대로 가져옴
- 파라미터 값을 가져와서 freeze 시킴
- 데이터가 많으면 freeze 영역을 줄여도 된다.
- 과도하게 train하지 않게 learning rate를 작게한다.

### ImageNet

- 20000개의 class 천만여개의 data

- FC layer를 윗부분(Top)으로 간주

### EfficientNet

- 효과적임
-
