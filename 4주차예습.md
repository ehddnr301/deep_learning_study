# 4주차 예습

## C1W3L6

- activation의 미분계수가 0에 가깝다

  - = activation의 접선 기울기가 0에 가깝다
  - = dW의 감소가 잘 일어나지 않는다.
  - = gradient descent 가 잘 일어나지 않는다.
  - = 학습이 일어나지 않는다.

- Saturation : weight의 update가 잘 일어나지 않는 현상
- vanishing gradient : gradient가 0에 가까워지는 현상

### output layer 의 activation function

- Binary classification : sigmoid
- multi-class classification : softmax

### Activation Functions

- 은닉층과 출력층에서 어떤 활성화 함수를 써야할지
- ex ) 시그모이드 function
  - a = 1 / 1 + e^-z
- ex ) tanh function

  - a = (e^z - e^-z) / (e^z + e^-z)

- hidden layer에 대해 tanh 를 사용하면 거의 항상 시그모이드 함수보다 좋습니다.
- 출력층을 계산할때에

  - binary classification 때는 시그모이드 쓰면 좋음

- hidden layer : tanh
- output layer : sigmoid

### ReLU

- a = max(0,z)
- hidden layer 에 사용하면 됨.
- 단점은 z가 음수이면 도함수가 0임

- 장점 : 대부분의 z에 대해 기울기가 0과 매우다름
- 신경망은 훨씬 빠르게 학습 가능
  - 학습을 느리게하는 원인인 함수의 기울기가 0에 가까워 지는것을 막아줌

### 각함수 정리

- sigmoid : binary classification의 output layer말고는 쓰지않는것이 좋다.
- tanh가 대부분의 경우에 더 좋음
- ReLU or Leakey ReLU(a = (0.01z, z))

## C1W3L7

- 왜 신경망은 비선형 활성화 함수가 필요할까?

- 선형활성화 함수나 항등활성화 함수를 쓰게되면
  - 신경망은 입력의 선형식만을 출력하게 됩니다
  - 은닉층이 없는것과 다름없다
  - ????????
  - 이모델은 표준 로지스틱 회귀 모델과 달라지지 않습니다.
  - 선형 hidden layer는 쓸모가 없다.
  - y가 실수값이라면 output layer에 대해 선형 활성화 함수를 써도 괜찮다.
    - 출력값인 y햇이 -무한대 ~ 무한대 가 될수있도록

## C1W3L8

- 신경망의 역방향 전파를 구현하려면 활성화 함수의 도함수를 구해야합니다.
  - 도함수? : 미분함수

### 활성화 함수별 도함수

#### sigmoid

- g(z) = 1 / (1 + e^-z)
- 도함수를 구하개되면

  - d/dz g(z) = g(z) (1- g(z))

- 신경망에서 a = g(z) 입니다.
- a = a(1-a)

#### tanh

- d/dz g(z) = 1 - (tanh(z))^2
- a = 1-a^2

#### ReLU

- g(z) = max(0,z) 도함수
  - z가 0보다 작을때 0
  - z가 0보다 클때 1

#### Leaky ReLU

- g(z) = max(0.01z,z) 도함수
  - z가 0보다 작을때 0.01
  - z가 0보다 클때 1

## C1W3L11

- `np.random.randn`
  - 가우시안 표준 정규 분포에서 난수 생성
- `질문 : w가 가중치 인데 아무렇게나 초기화 시키면 잘못된 결과를 학습하게 될수있는게 아닌지 ?`
