# 7주차 예습

## C2W1L9

- 신경망 훈련을 빠르게 할수있는 방법은 입력을 정규화 하는것입니다.

### 어떻게 입력을 정규화 ?

- 입력에다가 평균을 빼줘서 0근처로 맞추기.
- 0의 평균을 맞추도록 x = x-u

### 분산을 정규화

- x = x / 시그마^2
- x1,x2 두 특성 의 분산은 모두 1과 같습니다.

### 정규화 할때 명심

- 훈련세트를 정규화 할때 사용한 뮤(u) 와 시그마 를
- 테스트 세트를 정규화 할때도 똑같이 사용해야 합니다.

### 왜 입력을 정규화 ?

- 만약에 특성 x1 이 1-1000 범위 이고
- 특성 x2 가 0-1 범위라면
- w1과 w2가 매우 다른범위를 가지게 될것입니다.
- 하지만 특성을 정규화 하게된다면 cost function은 대칭적인 모양을 가지게 될것입니다.

- 그렇게 되면 gradient discent가 전진하기 쉬워집니다.
- 특성이 비슷한 범위를 갖는다면 정규화가 그다지 필요 없을 수 있습니다.
  - 하지만 정규화를 하는것은 어떤 나쁜영향도 끼치지 않기때문에 하는것을 추천!

## C2W1L11

- 아주 깊은 신경망에서 기울기 값이 0에 너무 가까워지거나 1에 너무 가까워 지는 현상이 있다.
- 이런 현상을 피하기 위해서 신경망에대한 무작위 초기화를 더 신중하게 선택하는 것이 좋습니다.

### 어떻게 ?

- z = w1x1 + w2x2 + ... + wnxn

  - n의 값이 커질수록 w는 작아져야 합니다.

- w^l = np.random.rand(shape) \* np.sqrt(1/n^(l-1))
  - relu를 activation으로 사용하는 경우 2/n^(l-1) 을 사용합니다.

#### 왜 n^(l-1) 인가요 ?

- 층 l 은 일반적으로 해당층의 각 유닛에 대해 n^(l-1) 입력을 갖습니다.

## C2W2L1

### Mini batch gradient descent

- 훈련샘플이 5,000,000 개 라면 ?

  - 다음 gradient discent 를 처리하기 위해서 오백만개의 훈련샘플을 처리해야 합니다.

- 훈련세트를 더 작은 훈련세트들로 나눴다고 생각합시다.

  - 이러한 작은 훈련세트를 mini batch 라고 합니다.

- 이러한 mini batch 를 표현할때
  - x1 - x1000 = X^{1}
  - x1001 - x2000 = X^{2}
  - x2001 - x3000 = X^{3}
- 이렇게 나눈다면 5000 개의 mini-batch 를 가지게 된다.
- y도 마찬가지로 Y^{1}, Y^{2} ... Y^{5000} 이게 됩니다.

- x^i = i번째 훈련샘플입니다.
- z^l = l번째 layer의 z 값을 나타냅니다.
- X^{t} = t번째 mini batch 라고합니다.
  - X = (n_x, m) 의 shape을 가지게됩니다. mini batch 의 갯수가 m 이 됩니다.
  - Y = (1, m) 의 shape을 가지게됩니다. mini batch 의 갯수가 m 이 됩니다.

## C2W2L2

- Batch gradient discent 의 cost 는 감소하는 그래프를 그립니다.
- Mini batch gradient discent 의 cost 는 전체적으로 감소하는 흐름이지만 노이즈가 끼게 됩니다.

  - 모든 반복에서 아래로 내려가지는 않습니다.

- 작은 훈련세트라면 그냥 nomal batch gradient discent 를 사용 ( 샘플이 2000 개 보다 작은경우)
- 일반적인 경우 mini batch 의 크기는 64혹은 128정도가 적당합니다.
- 2의 제곱형태인 경우가 코드를 빠르게 실행시켜 줍니다.
- CPU 와 GPU 에 맞는 mini batch 사이즈를 결정하길 바랍니다.
