# 3주차 예습

## C1W2L13

- `z = w.T * x + b` 라고 할때
- `Z = [z1  z2  z3  ... zm]` 입니다
- X 가 x를 가로로 쌓아 나간것 처럼
- Z 를 z를 가로로 쌓아 나간 것이라 한다면

- `Z = np.dot(w.T, X) + b`
  - w.T : (1, nX)
  - X : (nX, m)
  - 파이썬 에서는 실수 b 를 자동적으로 (1,m) 백터로 바꾸어 줍니다.


- 예측값 a 에서
- `a = 시그마(z)` 라고 한다면
- `A = [a1  a2  a3  ... am]` 입니다.
- `A = 시그모이드(Z)` 라고 할수있습니다.

- 포워드 프로파게이션이었습니다.

## C1W2L14

- `dz = a - y` 입니다.
- `dZ = [dz1 dz2 ... dzm]` 이라고 할 수 있습니다.
- `dZ = A - Y` 가 됩니다.


- `dw = X * dz.T   /   m`
- `dw = xi * dzi`

- `db = np.sum(dZ)  /   m`


- 최종적으로
  - w := w - 학습률알파(dw)
  - b := b - 학습률알파(db)

## C2W3L02

- 신경망 구성
  - 제일 왼쪽부분은 신경망의 입력층
  - 중간부분은 신경망의 은닉층 : 은닉층은 훈련세트에 기록되지 않습니다.
  - 마지막은 신경망의 출력층
  - 몇층 신경망인지 적어줄때 입력층은 카운트 하지 않습니다.

- 로지스틱 회귀에서는 ?
  - `z = w.T * x + b`
  - `a = 시그모이드(z)` 를 계산했습니다.

- 훈련샘플이 하나일때 ?!
- 은닉층이 하나인 신경망에서는 ?
  - 은닉층의 네 로지스틱 회귀의 출력 한번
    - `z1 = W1a0 + b1`
    - `a1 = 시그모이드(z1)`
  - 출력층의 로지스틱 회귀 출력 한번
    - `z2 = W2a1 + b2`
    - `a2 = 시그모이드(z2)`

- 훈련샘플이 m개일때 !?
- 은닉층이 하나인 신경망에서는 ?
  - `z1 = W1a0 + b1`
  - `a1 = 시그모이드(z1)`
  - `z2 = W2a1 + b2`
  - `a2 = 시그모이드(z2)`
  - 과정으로 훈련샘플 하나를 구한다고 했을때
  - for i range(m)으로 모두 구하면 된다.

- 위의 것을 백터화 한다면 ?
  - `Z1 = W1A0 + b1`
  - `A1 = 시그모이드(Z1)`
  - `Z2 = W2A1 + b2`
  - `A2 = 시그모이드(Z2)`
  - X = x를 쌓아서 (nx, m) 행렬 인것처럼 Z,A 도 z,a 를 쌓아서 만든것
    - 세로는 신경망의 노드들 
    - 가로는 훈련샘플의 번호들
  

### 신경망 구성 

- 첫번째 은닉층으로 나아갈때 입니다.
  - x1, x2, x3 .... 처럼 x가 입력값으로 들어옵니다.
  - 그 x가 은닉층에서
    - `z = w.T * x + b`
    - `a = 시그모이드(z)`
    - 와 같은 계산을 거칩니다.
  - a = 에측값 y햇 입니다.

### 첨자

- 위 첨자는 layer           `[첫번째 hidden layer 이면 1 두번째 hidden layer 이면 2]`
- 아래 첨자는 node in layer  `[layer 안에서 첫번째 node이면 1 두번째 node이면 2] `

