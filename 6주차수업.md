# 6주차 수업

- 데이터 양 많아야함
- table data 말고 unstructured data가 좋다.

- 데이터 분할에는 train_test_split 을 많이사용
  - shuffle : 라벨순으로 묶여서 나열된 데이터 같은 경우에 한번에 들어가는 데이터가 똑같아서 잘못된 방향으로 갈수도있음.
  - stratify : train data 와 test data 가 분포가 같아야함.
    - 고양이만 6천개 학습하고 고양이가 아닌 4천개를 test하면 잘 안될수밖에.
  
- model = Sequential() 로 할때 주의
  - input dim 을 feature의 갯수로 집어넣어준다.
  - output layer의 노드갯수 : 
    - binaray classification : 1 로 넣어줘야함.
    - regression : 값을 예측하는 건데 범위가 실수 전체범위 scalar 값이 나온다. : output node 1개
    - multiclass : 1이 아니어도 된다? 0~9 판별이면 10넣어줌. : class의 갯수가 output layer node 갯수
  - activation : 

## 강의

### softmax

- multi-class classification
- softmax를 거치면 노드별로 특정 class에 속할 확률값이 나옵니다.

## one-hot encoding

- 특정 카테고리 자체가 값인 변수 (cat, dog ...)
  - 머신러닝에 넣을수 없으니까 벡터형태로 바꿔줘야함.
cat
```
1
0
0
0
```
dog
```
0
1
0
0
```
cow
```
0
0
1
0
```
- 이런식으로 됨.
- 독립적인 형태로 하려고 벡터형태로 표현
  - 0,1,2,3,4 이런식으로하면 2.7이면 3으로 에측해야하나? 그건아님 개도아니고 고양이도 아닌데 고양이로 예측해버릴수도있음.

## categorical cross entropy loss




## 강의 1-2

- underfitting인지 overfitting인지 성능을 보고 파악.
  - 어떤 해결책을 써야할지 ?

- high bias
  - underfitting
  - 학습시간 늘림, 더 긴시간 데이터 학습, 히든레이어 갯수 늘림, 해결책이 다양함.

- high variance
  - overfitting
  - 더많은 데이터 학습, Regularization(정규화), 신경망 개선?
  - dropout : epoch으로 여러번 학습하는데 노드를 랜덤으로 삭제해서 학습함. (좋음)
  - augementation : 데이터 증폭 : 고양이 사진을 회전, 반전 시켜도 고양이 이므로 사용. (이미지에 많이 사용)
  - 앙상블 : 모델들을 하나만 만드는게 아니라 여러개를 만듬 그래서 평균을냄 (컴퓨터성능 많이필요해서 별루,,,)

## 강의2

### 정규화

- regularization : 정규화 : 모델이 과도하게 복잡해지는것을 막음.
  - w 값을 줄여서 함.
  - hidden layer의 효과가 사라진다.
  - overfitting 된걸 줄이기 위해서 사용한다. (너무 람다 값을 크게하면 underfitting현상이 발생한다.)

### dropout

- 은행에 갈때마다 은행직원이 매번 바뀌어서 왜 그런가 했더니
  - 부정행위를 저지를것을 방지하기 위해서 바뀐다.

- 매 epoch 마다 node를 랜덤하게 제거해가면서 학습을 시켰다.

- keep_prob : layer node를 몇퍼센트 확률로 남기냐
- drop_late : 얼마나 내가 제거할것인지에 대한 비율. 0.2

- np.random.rand(a.shape[0], a.shape[1]) < keep_prob
  - boolean 값을 가짐.
  - `곱셈연산시에 True False 는 1 과 0 으로 바뀌어서 계산된다.`
- inverted_dropout : keep_prob으로 나눠서 a3의 예상값이 바뀌지 않게한다가 무슨의미인지..?
  - 노드를 삭제하면 문제가 발생
  - 노드가 0.5로 삭제가되면 z값이 절반가량 줄어든다.
  - 그래서 그것을 보정해주려고 하는거임.

- dropout이 왜 효과가 있냐
  - epoch 마다 삭제되는 노드를 변경해준다.
  - 그러면 작은 모델들의 앙상블이 됨. (과도하게 학습되는것을 막아줄수있음.)
  - 고양이 귀에 꽂혀서 학습되지 않게 귀를 지운거도 학습시켜서 얼굴형을 학습한다던가...

### dropout 규칙.

- keep_prob : 각 레이어마다 다르게 설정가능.

## regularizer, dropout

- drop out을 보통 0.2~ 0.5 를 한다.
  - drop out 적용하고자 하는 레이어 바로 밑에다가 써줘야함 model.add(Dropout(0.2)) 이런 형태로 바로 밑에!

- 최종적으로 과제 모델을 만들었을때
  - 파란색이 기본모델
  - 주황색이 regularization 쓴거
  - 녹색이 dropout 쓴거
  - 실선이 train
  - 점선이 test
  - 실선 점선 차이가 최적화를 진행하면 줄어듬 ( overfitting이 줄어듬 )
    - 그렇다고 성능차이가 엄청난게 아니다.
  - 모델 만들때 오버피팅이 발견되었을때 dropout 등을 적용해야함. 먼저 train을 끌어올려야함.