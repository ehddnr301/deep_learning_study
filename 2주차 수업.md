# 2주차 🚀


## 로지스틱 리그래션

- 머신러닝에서 절반이상 차지
- 여기서 딥러닝
- 꽤자세하게 다룰예정
- binary classification 을 위한것
- 이미지를 넣어주고 이미지를 cat으로 인식하면 1 아니면 0을 출력하게함
- 64 x 64 픽셀 이면 rgb이니까 64 * 64 * 3 개의 픽셀 데이터 = 핏쳐갯수 12288
- 붉은색만 일렬정렬 -> 녹색 일렬정렬 -> 파랑 일렬정렬 : 64 * 64 * 3개
- `Nx : 12288 : n` : 인풋 feature 갯수
- 이미지에서 feature : 특성 : rgb에서 feature의 갯수는 : 가로픽셀갯수 * 세로픽셀갯수 * rgb이니까 3

- x 는 feature vector
- y 는 label
- m 은 테스트 샘플의 갯수
- X : 행은 m 열은 : Nx
- X.shape = (Nx,m)
- Y.shape = (1,m)

- 학습은 최적의 w,b 를 찾는것
- 코스트 값을 가장 작게하는 w,b 를 찾는다.

### 5가지 과정 중요중요중요

- `데이터는 주어져있는데 처음시작하는 점이 정해져있지 않다. (x,y)`
- 초기점을 잘 정하면 빨리 도달한다.

- `w도 열이 1개 x도 열이 1개니까 곱하려면 w를 transpose 시켜주는거임.`
  - x1w1 + x2w2 + ... + xnxwnx + b

- `binarary 에서 치역의 범위를 0-1 로 만들어주기위해 시그모이드를 사용한것`

- w,b 를 0,0 으로 시작 zero initialization

- J(w,b) w = w - 알파 * dJ/dw
- 1. initialization
  - w,b 값 정하는것 ? 
- 2. 포워드 프로파게이션
  - 치역이 0과 1을 넘는걸 시그모이드적용으로 해결
  - y햇 이 0.5보다 크면 1로 predict
  - y햇 이 0.5보다 작으면 0으로 predict



#### 두번째 과정



### 이미지

- x : data 1개의 input = feature vector (열의 갯수)
- y : data 1개의 label = 0 or 1 의값 (binary classification 에서)
- `(x,y) : training data`
- 데이터가 m 개 있다.
- 데이터 갯수에대한 index
  - 1 - nx 개 까지 있으면 이걸 x1으로 인덱싱
- 벡터라이제이션 (vectorization)
- 행의갯수 nx개 / 열의 갯수 : m개
  - (nx, m)
  - `이미지 10개가 있으면 열의 갯수 m = 10, 행의갯수 = 가로픽셀갯수 * 세로픽셀갯수 * 3(rgb)`
- flatten = 이미지를 rgb 로 나눠서 한줄로 세움

### Binary Classification

- 수퍼바이즈드 러닝
  - discrete
    - binary classification
    - multiclass classification
  - continuous

#### 이미지 하기 전에 배경지식

- image 데이터는 내부가 숫자로 되어있음. pixel값들이 존재 pixel값이 0~255
- 검은색이 0 흰색이 255
- color : 3개의 채널     (컬러이미지)
- grayScale : 1개의 채널 (흑백이미지)

- unstructure data : 
- structure data : excel로 정리가능 (집값예측, )
- 데이터를 읽을수 있으려면 숫자여야함 : 이미지는 숫자가 아니다  그럼 어떻게 기계에 넣어줌 ?
  - pixel값을 받아서 그대로 넣어줌
- `1d-array` 여야 한다 (pixel 을 넣어주고 flatten 시켜준다.) 순서대로 넣어서 세로로 한줄 나오게 넣어준다.
- 빨간색값 한줄로 쭉 넣고 초록색 쭉넣고 파란색 쭉넣고

```
100 // 빨강시작
10
4
52 
234 // 초록시작
```





# 2

## 5가지 과정 중요중요중요


- `시그마 * (wTx + b) : 포워드 프로파게이션` 


- 로지스틱 리그래션
- x 도 원소를 nx개 / w도 원소를 nx개 가짐.
- ### x를 다음으로 넘겨줄때 곱해주는 가중치 `w`
- 그러니까 x의 갯수랑 w의 갯수가 같은거임

- `데이터는 주어져있는데 처음시작하는 점이 정해져있지 않다. (x,y)`
- 초기점을 잘 정하면 빨리 도달한다.

- `w도 열이 1개 x도 열이 1개니까 곱하려면 w를 transpose 시켜주는거임.`
  - x1w1 + x2w2 + ... + xnxwnx + b

- `binarary 에서 치역의 범위를 0-1 로 만들어주기위해 시그모이드를 사용한것`

- w,b 를 0,0 으로 시작 zero initialization

- J(w,b) w = w - 알파 * dJ/dw
- 1. initialization
  - w,b 값 정하는것
- 2. 포워드 프로파게이션
  - `y햇 = 시그마 (w^T * x + b)`
  - 치역이 0과 1을 넘는걸 시그모이드적용으로 해결
  - y햇 이 0.5보다 크면 1로 predict
  - y햇 이 0.5보다 작으면 0으로 predict
- 3. 코스트 펑션
  - y햇과 y의 차이를 줄이기 위해 코스트 펑션이 필요?
  - y햇은 시그모이드 함수 값이기 때문에 1보다 클수없습니다.
    - y가 1일경우 y햇 이 1보다 클순없으므로 1에 수렴하길 원한다
    - y가 0일경우 y햇 이 0보다 커야하므로 0에 수렴하도록 매개변수조정
    - 로스는 데이터 하나에대한 로스값이고
    - 코스트는 데이터 m개의 로스값의 평균
    - cost 1/m 시그마 (로스)
    - 바이너리, 멀티클래스 둘다 로스펑션이 다름
- 4. backward propagation
  - gradient descent에서 w 즉 가중치를 업데이트 해주는건데
  - gradient descent 를 사용하기 위해서 해주는 chain rule?
- 5. gradient descent
  - w = w - 알파 * (dJ/dw)
    - 알파 : learning rate
    - 문제는 dJ/dw 즉 gradient는 어떻게 구하냐



## Chain rule

- 합성함수 미분법
  - `dJ/dw = dJ/dy햇 * dy햇/dz * dz/dw`


## 7 & 8강

- J(a,b,c) = 3(a+bc)
  - u = bc
  - v = a+u
  - J = 3v
    - J = 3v = 3(a+u) = 3(a+bc)
    - J = 3(a+bc) = 3(a+u) = 3v
- 최종 목표는 dJ/da 구하기라면 ?
  - dJ/dv * dv/da  = 3 * 1

- `dL/dw1 = dw1 으료 표기`

- J를 w에 대해 미분한게 L을 w에 대해 미분한것을 다더해서 m으로 나눈것이랑 같음

- 다 더해서 

# 12강

- np.exp : 시그모이드에서 1/1+e^-z 가 있어서 나타남
  - 익스포넨셜 : 자연상수 e

- np.exp(1d_array)

