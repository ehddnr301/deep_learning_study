# 4주차 수업

array([[ 0, 1, 2, 3, 4, 5],
[ 6, 7, 8, 9, 10, 11],
[12, 13, 14, 15, 16, 17],
[18, 19, 20, 21, 22, 23],
[24, 25, 26, 27, 28, 29],
[30, 31, 32, 33, 34, 35]])

## slicing

- `[start:end:step]` : start 이상 end 미만. (step 만큼 증가)

## numpy

- 쉼표사용가능.
- `a[3:6, 3:6]`

## Fancy indexing

- `a[[2,5]]` - 대괄호 2개 - (2행과 5행) (continuous 한 slicing이 아니라 내가 원하는 특정 행만 선택)

```
array([[12, 13, 14, 15, 16, 17],
       [30, 31, 32, 33, 34, 35]])
```

- `a[[2,5], [2,5]]` # a[2,2]와 a[5,5]를 indexing 하는 것과 같다.

```
array([14, 35])
```

- `a[(2,5), (2,5)]`

```
`array([14, 35])`
```

## 3강 & 4강

- 노드를 여러개 확보하면 성능 향상 가능
  - 히든레이어에 있는 노드갯수를 어떤방식으로 늘려서 성능 향상 ?
  - hyperparameter가 몇개일때 좋나요 ? 알수없다가 정답
  - 노드를 늘리니까 성능이 좋아지는 경향은 있다.
  - 성능을 보면서 해야함.
- w1.T , w2.T , w3.T , w4.T

- wi^l = `i번째 노드` `l번째 layer`

* w의 행의갯수는 히든레이어의 노드 갯수만큼 있다.

  - 열의 갯수는 feature의 갯수

- `n^[l]` = l번째 layer에 있는 node의 갯수를 의미.

### 중요

- W X + b = Z
- X에서 행은 feature의 갯수 n_x개, 열은 data 갯수 m개
- `(node갯수, n_x) (n_x, m) + (node갯수,1) =`
- `(n^[1], n^[0]) (n^[0], m) + (n[1], 1) = (n[1],m)`
- `W X b`

중요중요중요중요중요중요중요중요중요중요중요중요중요중요중요중요중요중요중요중요중요중요중요중요

- W = (노드의 갯수, feature의 갯수)
- X = (feature의 갯수, 데이터 갯수)

중요중요중요중요중요중요중요중요중요중요중요중요중요중요중요중요중요중요중요중요중요중요중요중요

## 기울기

### sigmoid

- `g(z) = 1/ (1+e^-z)`
- `g'(z) = g(z) (1-g(z))`
- `g'(z) = a (1-a)`
- 미분계수의 최대값이 1/2

### tanh

- `g'(z) = 1- (tanh(z))^2`
- `g'(z) = 1 - a^2`
- 미분계수의 최대값이 1
  - sigmoid 보다는 0에 가까운 구간이 적다

### ReLU

- `g(z) = max(0,z)`
- `g'(z) = 0 (when z <0 )`
- `g'(z) = 1 (when z >0 )`

### Leaky ReLU

- ReLU 보다는 비선형성이 덜하다.
- 전구간에서 연산을 해야하므로 ReLU보다 느릴수밖에 없다.

### 의문

- 그냥 y=x 형태의 function을 쓰면 되지않는가 ?
- 왜 non-linear 한 function을 써야만 하는가 ?

#### 선형함수는 나쁘다.

- layer를 아무리 쌓아도 layer가 선형레이어 하나인거와 똑같은 효과.
- 깊어져야 딥러닝인데 깊어질수가 없다.

#### non-linear

- [http://neuralnetworksanddeeplearning.com/chap4.html]
- 왜 non-linear ?
- 내가 원하는 data만 백터스페이스를 변형시켜서 분류하기 편하게 만듬.

#### 학습이 잘 안일어난다

- cost 가 멈춤

## initialization

- 0 으로 초기화하게되면 어떤문제가 발생하는가 ?
  - 각 element가 0이어서 생기는문제.
  - 각 element가 모두 같은 값이어서 생기는 문제.
    - 최종적으로 연산되서 나오는 값도 같은 값
    - row symmetric : 각각의 행이 같게 된다.
    - 전부다 row symmetric 이 발생
    - 업데이트 하는 parameter도 같은 문제 = 노드별로 업데이트 하는게 같다 = 노드가 하는 역할이 똑같다 = 여러개나 하나나 차이가 없음
    - 결국 node가 여러개인 효과를 발휘하지 못한다는 뜻.

### 결론

- w = `0.01 * np.random.randn(shape)`
- b = 0

### w,b 가 잘못 initialization

- 이것은 initialization 에서 극복하는 문제가 아니라 w를 업데이트 하는부분에서 극복해야 하는 문제.

### 3주차 과제는 왜 잘 작동했는가

- layer 하나 짜리였기 떄문.
