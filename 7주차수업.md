# 7주차 수업

## 1

### standardization

- minmax_Scaler : 0-255 를 0-1 로 바꾸기

- 만약에 특성 x1 이 1-1000 범위 이고
- 특성 x2 가 0-1 범위라면
- w1과 w2가 매우 다른범위를 가지게 될것입니다.
- 하지만 특성을 정규화 하게된다면 cost function은 대칭적인 모양을 가지게 될것입니다.
- gradient discent를 하게되는데 이경우에 x1에 맞춰서 하게되면 너무 빠르게 일어나게 된다.

### Random Initialization의 문제점

- fan-in : 들어오는 레이어가 이전 레이어의 node 갯수
- fan-out : 현재 레이어의 node 갯수

- fan-in 이 클수록 fan-out의 크기가 커집니다. : gradient exploding
- fan-in 이 작을수록 fan-out의 크기가 작아집니다. : gradient vanishing

#### 어떻게 개선 ?

- fan-in 과 초기화 상수를 반비례 시킨다.
- fan-in 이 클수록(많을수록) w의 가치가 작아지길 바람.

##### Xavier initialization

- 과제에서 사용한 glorot initialization 이다. default 값으로 설정되어 있음.
- relu에서는 잘 안될수도있음.
  - 0에 w값들이 과도하게 몰릴수있다.

##### He initialization

- relu계열에 특화
- fan-in 특화

### initialization 에따른 학습

- zeros : 학습이 이어져도 정확도가 멈추는것을 확인할수있습니다.
- random : 많이 오르지 않음.

## 2

### mini batch

- batch : data 전체
  - 메모리 부족때문에 한번에 다 처리할수가없다.
- mini batch : hyperparameter 이다.
  - 모집단에서의 cost function과 표본에서의 cost function이 다를수있다.
  - 하지만 전반적인 방향은 원래의 cost와 일치.
  - 적절히 사용되면 가장 빠르다.
- 어떻게 미니배치의 크기를 선택하는가 ?

### batch_size

- `fit 에서 설정가능`
- mini batch 를 의미합니다.
- hyper parameter
- 단일 iteration 에서 gradient descent 하는데 사용하는 data 갯수.
- 얼마로 설정해야해 ?
  - 64-512 사이에서 선택하는데 2의 거듭제곱수로 선택하라.
  - 최소값은 보통 2^5 = 32 이다. :
  - 최대값은 보통 크면클수록 좋은데 내 컴퓨터 사양을 초과하면 out of memory 라는 에러가 난다.
  - 훈련세트가 작으면 그냥 batch를 사용하라 (batch_size 설정할 이유가 없음.)
  - CPU,GPU 메모리에 맞게 선택하라.

### optimizer

#### learning rate

- 꽤 중요한 hyperparameter
- 낮추려면 10^-2 => 10^-3 이런식으로 낮춘다.

#### SGD

- random하게 추출한 mini-batch씩 gradient descent 진행한다
- gradient가 dimension 별로 차이가 크면 minimum을 찾기 어렵다.

- local minimum (분지?) 혹은 plateaus (골짜기 혹은 고원)
  - (즉, 일시적으로 gradient가 0에 가까운 지점)
  - => gradient descent 멈춘다.
  - 관성을 반영해서 해결가능.
    - momentum사용
      - momentum 이 global minimum 도 탈출하는 문제점 발생

* gradient discent 에서 봤을때
* w = w - 알파dw
  - dw : 기울기
  - 알파 : 크기

### 3

- 1. Normalization
     분야마다 normalization이라는 용어가 많다.

여기서는 scale을 0과 1사이로 만들어주는 것을 의미

- 2. Minmaxscaler
     해당 data를 원하는 범위로 조정하는 것 (min과 max를 파악해서 조정하면 된다.)

보통 scale을 0과 1사이로 만든다. (default)

minmaxscaler에서 feature_range를 0과 1사이로 만드는 것 == normalization

결과: feature의 분포가 0과 1사이 안으로 들어온다.

- 3. Standardization
     고등학교 통계과목에서 배운 표준화를 의미

해당 data feature의 평균과 표준편차를 구해서 '(feature 값 - 평균) / 표준편차' 연산을 가해준다.

결과: feature의 분포가 표준정규분포를 따른다. = N(0,1) (standard normal distribution)
