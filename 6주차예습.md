# 6주차 예습

## C2W1L3

### overfitting 을 줄이기위해

- 정규화 시도
- 다른 신경망 아키텍쳐를 찾아볼수 있음 ( 다른신경망 아키텍쳐가 뭘 의미 ??)
  - 다른 hyperparameter를 가진걸 의미하는건가 ?

## C2W1L4

### Regularization

- 높은 분산으로 신경망이 데이터를 overfitting 하는게 의심된다면 시도해야 할것입니다.
- 정규화 매개변수 `람다` 를 로지스틱회귀에 추가합니다.

### L2 regularization

- (람다 / 2m) x (w.T x w) 를 추가해줍니다.

#### Nerual network에서는 ?

- dw^l = ~ + (람다/m)w^l
- 가중치 감쇠라고도 불린다.

## C2W1L5

- 왜 정규화가 overfitting에 도움이 되는가.
  - hidden unit 들의 영향력을 0 에 가깝게 줄임으로써 로지스틱 회귀에 가까운 네트워크를 만드는것

## C2W1L6

### Dropout

- 노드를 무작위로 삭제해서 더작은 네트워크를 훈련시킴.

#### inverted dropout

- 층이 3개인 경우
  - keep_prob = 0.8 을 하면 0.2의 확률로 삭제된다.
  - d3 는 bool 타입.
  - keep_prob 을 어떤값을 하던지 a / keep_prob 을 해줌으로써
  - a3의 기대값을 같게 유지합니다.
- 테스트할때는 dropout을 하지않는다.

## C2W1L7

- 왜 dropout이 잘 작동할까

  - 어떤 한가지 feature에 대해서 가중치가 치우치지 않기때문에
  - overfitting을 줄이는데 효과적.

- 층마다 keep_prob을 다르게 설정가능.

  - `근데 우리가 그 안의 hidden layer가 어떤 역할을 하는 layer인지 모르는데 교수님 말씀대로 가중치를 줄이는게 효과적인지 아닌지 어떻게 아는지 ? 아니면 해보고 효과가 좋아지면 줄이고 아니면 가중치를 다시 늘리는 식의 hyperparameter 개념인건지`
  - `우리가 그 layer가 overfitting의 우려가 많은 층이라는것은 결과를 보고 알수있는 것인지 ?`

- input은 dropout 하지 않는게 나음.

- `dropout이 overfitting을 해결해주는데 앞서서 수업에서 나온거처럼`
- `에폭을 증가시키면 결국에는 똑같아 지는건가?`

## dropout 단점

- cost function j 가 잘 정의되지 않음.
